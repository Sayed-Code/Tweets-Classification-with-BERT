# Tweets-Classification-with-BERT
# Main Objectives and Related Research Questions  
Tweet text classification with BERT, XGBoost and Random Forest.  
Text Categories: Hate, Offensive, Profanity or None. 

Research Questions:  
1. How would attention-based models perform when fine-tuned and tested with different dataset combinations from different time periods?  
2. Is the models' performance independent of the language used ?  
3. Is BERT a better solution than traditional machine learning models for text classification 
 
 

# Datasets  
HASOC 2019  
HASOC 2020    
link to data sources: [HASOC DATASETS MAIN SOURCE](https://hasocfire.github.io/hasoc/2020/dataset.html)

# BERT MDOELS CONSIDERED  
1. [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased)
2. [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased)
3. [bert-base-german-cased](https://huggingface.co/bert-base-german-cased)

# Classification Pipeline  
![Classification Pipeline](methods.PNG)
